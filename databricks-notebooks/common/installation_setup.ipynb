{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e01d9cad-2d20-4bb4-bb8e-be56bdd14102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog Name: llm_workspace\nSchema Name: default\nWorking Directory: /Workspace/Shared/genai-databricks-hackathon-2024/databricks-notebooks/common\nGeneral Volume Name: /Volumes/llm_workspace/default/regubim-ai-general-volume/\ninstall_env.sh\nroom-relationship-visualisation-min.html\nroom-route-visualisation-min.html\nRegulation Volume Name: /Volumes/llm_workspace/default/regubim-ai-regulation-data-volume/\nBIM Volume Name: /Volumes/llm_workspace/default/regubim-ai-bim-data-volume/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "catalog_name = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "schema_name = spark.catalog.currentDatabase()\n",
    "working_directory = os.getcwd()\n",
    "\n",
    "general_volume = f\"/Volumes/{catalog_name}/{schema_name}/regubim-ai-general-volume/\"\n",
    "regulation_volume = (\n",
    "    f\"/Volumes/{catalog_name}/{schema_name}/regubim-ai-regulation-data-volume/\"\n",
    ")\n",
    "bim_volume = f\"/Volumes/{catalog_name}/{schema_name}/regubim-ai-bim-data-volume/\"\n",
    "\n",
    "print(f\"Catalog Name: {catalog_name}\")\n",
    "print(f\"Schema Name: {schema_name}\")\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "\n",
    "\n",
    "def print_files_in_volume(volume_name):\n",
    "    files = dbutils.fs.ls(volume_name)\n",
    "\n",
    "    # Print the files and folders in the volume\n",
    "    for file in files:\n",
    "        print(file.name)\n",
    "\n",
    "\n",
    "print(f\"General Volume Name: {general_volume}\")\n",
    "print_files_in_volume(general_volume)\n",
    "\n",
    "print(f\"Regulation Volume Name: {regulation_volume}\")\n",
    "print_files_in_volume(regulation_volume)\n",
    "\n",
    "print(f\"BIM Volume Name: {bim_volume}\")\n",
    "print_files_in_volume(bim_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780b3bf7-e20c-4176-8d8c-89534a59558d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logs\n",
    "\n",
    "import time\n",
    "import re\n",
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "import warnings\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "import json\n",
    "\n",
    "# Handle potential import error with accelerate\n",
    "try:\n",
    "    pass\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: {e}. MLU support might not be available in this environment.\")\n",
    "\n",
    "# parse PDF bytes using PyPDF2\n",
    "def parse_bytes_pypdf(raw_doc_contents_bytes: bytes):\n",
    "    try:\n",
    "        pdf = io.BytesIO(raw_doc_contents_bytes)\n",
    "        reader = PdfReader(pdf)\n",
    "        parsed_content = [page_content.extract_text() for page_content in reader.pages]\n",
    "        return \"\\n\".join(parsed_content)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Exception {e} has been thrown during parsing\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def pprint(obj):\n",
    "    import pprint\n",
    "\n",
    "    pprint.pprint(obj, compact=True, indent=1, width=100)\n",
    "\n",
    "\n",
    "# display result nicely in panel\n",
    "def print_nested_dict_display(data):\n",
    "    console = Console()\n",
    "\n",
    "    def format_value(value):\n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                # Try to parse as JSON first\n",
    "                json_data = json.loads(value)\n",
    "                return Markdown(f\"```json\\n{json.dumps(json_data, indent=2)}\\n```\")\n",
    "            except json.JSONDecodeError:\n",
    "                # If not JSON, treat as Markdown\n",
    "                return Markdown(value)\n",
    "        elif isinstance(value, dict):\n",
    "            return Markdown(f\"```json\\n{json.dumps(value, indent=2)}\\n```\")\n",
    "        else:\n",
    "            return str(value)\n",
    "\n",
    "    for key, value in data.items():\n",
    "        formatted_value = format_value(value)\n",
    "        panel = Panel(formatted_value, title=key, expand=False)\n",
    "        console.print(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e881b1b7-ba0a-44a2-9df6-3853935da08d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "\n",
    "def index_exists(vsc, endpoint_name, index_full_name):\n",
    "    try:\n",
    "        dict_vsindex = vsc.get_index(endpoint_name, index_full_name).describe()\n",
    "        return dict_vsindex.get(\"status\").get(\"ready\", False)\n",
    "    except Exception as e:\n",
    "        if \"RESOURCE_DOES_NOT_EXIST\" not in str(e):\n",
    "            print(\n",
    "                f\"Unexpected error describing the index. This could be a permission issue.\"\n",
    "            )\n",
    "            raise e\n",
    "    return False\n",
    "\n",
    "\n",
    "def wait_for_vs_endpoint_to_be_ready(vsc, vs_endpoint_name):\n",
    "    for i in range(180):\n",
    "        endpoint = vsc.get_endpoint(vs_endpoint_name)\n",
    "        status = endpoint.get(\"endpoint_status\", endpoint.get(\"status\"))[\n",
    "            \"state\"\n",
    "        ].upper()\n",
    "        if \"ONLINE\" in status:\n",
    "            return endpoint\n",
    "        elif \"PROVISIONING\" in status or i < 6:\n",
    "            if i % 20 == 0:\n",
    "                print(\n",
    "                    f\"Waiting for endpoint to be ready, this can take a few min... {endpoint}\"\n",
    "                )\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"\"\"Error with the endpoint {vs_endpoint_name}. - this shouldn't happen: {endpoint}.\\n Please delete it and re-run the previous cell: vsc.delete_endpoint(\"{vs_endpoint_name}\")\"\"\"\n",
    "            )\n",
    "    raise Exception(\n",
    "        f\"Timeout, your endpoint isn't ready yet: {vsc.get_endpoint(vs_endpoint_name)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def wait_for_index_to_be_ready(vsc, vs_endpoint_name, index_name):\n",
    "    for i in range(180):\n",
    "        idx = vsc.get_index(vs_endpoint_name, index_name).describe()\n",
    "        index_status = idx.get(\"status\", idx.get(\"index_status\", {}))\n",
    "        status = index_status.get(\n",
    "            \"detailed_state\", index_status.get(\"status\", \"UNKNOWN\")\n",
    "        ).upper()\n",
    "        url = index_status.get(\"index_url\", index_status.get(\"url\", \"UNKNOWN\"))\n",
    "        if \"ONLINE\" in status:\n",
    "            return\n",
    "        if \"UNKNOWN\" in status:\n",
    "            print(\n",
    "                f\"Can't get the status - will assume index is ready {idx} - url: {url}\"\n",
    "            )\n",
    "            return\n",
    "        elif \"PROVISIONING\" in status:\n",
    "            if i % 40 == 0:\n",
    "                print(\n",
    "                    f\"Waiting for index to be ready, this can take a few min... {index_status} - pipeline url:{url}\"\n",
    "                )\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"\"\"Error with the index - this shouldn't happen. DLT pipeline might have been killed.\\n Please delete it and re-run the previous cell: vsc.delete_index(\"{index_name}, {vs_endpoint_name}\") \\nIndex details: {idx}\"\"\"\n",
    "            )\n",
    "    raise Exception(\n",
    "        f\"Timeout, your index isn't ready yet: {vsc.get_index(index_name, vs_endpoint_name)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_vs_endpoint(vs_endpoint_name):\n",
    "    vsc = VectorSearchClient()\n",
    "\n",
    "    # check if the endpoint exists\n",
    "    if vs_endpoint_name not in [e[\"name\"] for e in vsc.list_endpoints()[\"endpoints\"]]:\n",
    "        vsc.create_endpoint(name=vs_endpoint_name, endpoint_type=\"STANDARD\")\n",
    "\n",
    "    # check the status of the endpoint\n",
    "    wait_for_vs_endpoint_to_be_ready(vsc, vs_endpoint_name)\n",
    "    print(f\"Endpoint named {vs_endpoint_name} is ready.\")\n",
    "\n",
    "\n",
    "def create_vs_index(\n",
    "    vs_endpoint_name, vs_index_fullname, source_table_fullname, source_col\n",
    "):\n",
    "    # create compute endpoint\n",
    "    vsc = VectorSearchClient()\n",
    "    create_vs_endpoint(vs_endpoint_name)\n",
    "\n",
    "    # create or sync the index\n",
    "    if not index_exists(vsc, vs_endpoint_name, vs_index_fullname):\n",
    "        print(f\"Creating index {vs_index_fullname} on endpoint {vs_endpoint_name}...\")\n",
    "\n",
    "        vsc.create_delta_sync_index(\n",
    "            endpoint_name=vs_endpoint_name,\n",
    "            index_name=vs_index_fullname,\n",
    "            source_table_name=source_table_fullname,\n",
    "            pipeline_type=\"TRIGGERED\",  # Sync needs to be manually triggered\n",
    "            primary_key=\"id\",\n",
    "            embedding_source_column=source_col,\n",
    "            embedding_model_endpoint_name=\"databricks-bge-large-en\",\n",
    "        )\n",
    "\n",
    "        # vsc.create_delta_sync_index(\n",
    "        #     endpoint_name=vs_endpoint_name,\n",
    "        #     index_name=vs_index_fullname,\n",
    "        #     source_table_name=source_table_fullname,\n",
    "        #     pipeline_type=\"TRIGGERED\", #Sync needs to be manually triggered\n",
    "        #     primary_key=\"id\",\n",
    "        #     embedding_dimension=1024, #Match your model embedding size (bge)\n",
    "        #     embedding_vector_column=\"embedding\"\n",
    "        # )\n",
    "\n",
    "    else:\n",
    "        # Trigger a sync to update our vs content with the new data saved in the table\n",
    "        vsc.get_index(vs_endpoint_name, vs_index_fullname).sync()\n",
    "\n",
    "    # Let's wait for the index to be ready and all our embeddings to be created and indexed\n",
    "    wait_for_index_to_be_ready(vsc, vs_endpoint_name, vs_index_fullname)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "/Workspace/Shared/genai-databricks-hackathon-2024/databricks-notebooks/databricks_base_environment.yml",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "installation_setup",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
